
# healthcheck traffic spike

- Start: 5pm, 22 November 2013
- duration: 1.5 hours
- environment: PROD

## Summary

Increased traffic from the probes sent by the CDN to healthcheck the backend caused an corresponding increase in load on the Content API, rendering it unstable for
a short period of time. Removing the healthcheck traffic allowed the backend to recover and the stack to get itself back in to a healthy state.

## Resolution

We tailed the production nginx and article server logs and identified two URLs that were being hit several thousand times a minute.

These were identified as our healthcheck URLs.

We temporarily intercepted these two requests in nginx (remember folks, you can patch nginx in production very quickly), which had the effect of
reducing load on the Content API.

```
# nginx instruction to bounce a http 200 back at the client
location ~ ^/football/2010/jul/11/rafa-benitez-yossi-benayoun-liverpool$ {
    default_type text/html;
    return 200;
}
```

The Content API did not recover, so we switched back to CAPI v1 for five minutes, which allowed the v2 API to stabilse, after which we moved back to v2. 

_Nb. This instruction is now removed in favour of a lighter healthcheck. See the Actions section for more detail._

## User-facing consequences

An approximate 30% drop in traffic was observed on the Ophan dashboards throughout the duration of the incident.

We observed the fronts traffic (facia) and most of the recently published content remained accessible thanks to the Varnish grace period.

## Thoughts

- There is still little protection (caching etc.) between ourselves and the Content API. Traffic spikes, accidental or otherwise, are not preventable, as we
  keep seeing.
- Our build processes depends on a healthy production stack to pass the tests and produce the artefact. During times of instability it is therefore
  impossible to patch the software in the usual way (via Team City) without disabling the tests.
- The CloudWatch graphs remained useful throughout the issue. Yesteryear the Ganglia data would often disappear during the outage.
- Likewise, our dashboards reflected the state of the stack well.
- Our boxes performed fine under the increased load.
- Seemingly, the more nodes Fastly create, the more backend traffic we see. Shielding should help alleviate this problem. 

## Actions

- We need to isolate the CDN healthcheck from the stack - GK - DONE
- Run a trial of the shielding functionality on the CDN - GK
- _Actually_ get a caching proxy set up in between us and the Content API (rather than just talk about it) - MC and the CAPI team

